# Example 06: Validation and Gatekeeping

Demonstrates small local models as validation layers -- checking whether inputs are safe, compliant, or contain sensitive data before expensive downstream processing. This is the "bouncer at the door" pattern: fast, cheap, and good enough to filter the vast majority of cases.

## Scenarios

### Prompt Injection Detection
Classifies user prompts as safe or unsafe before passing them to a downstream LLM. Detects injection attacks, jailbreak attempts, and data exfiltration tries. 20 labeled prompts (10 safe, 10 unsafe across injection/jailbreak/data_exfiltration categories).

### PII Detection
Identifies whether text contains personally identifiable information (email, phone, SSN, address, name, credit card, passport, DOB). 20 labeled texts (10 with PII, 10 without). Distinguishes personal data from technical identifiers.

## Asymmetric Error Costs

Validation has asymmetric error costs:
- **False negatives** (missed unsafe/PII) are dangerous
- **False positives** (blocking safe content) are annoying but safe

The scoring reports recall, precision, and false positive rate separately so users can evaluate the tradeoff.

## Running

```bash
# Run both scenarios
go run . -model qwen3:4b

# Run a specific scenario
go run . -model qwen3:4b -scenario prompts
go run . -model qwen3:4b -scenario pii

# Score results
go run . -score

# Generate comparison report
go run . -report
```

Or from the project root:

```bash
make run-example EXAMPLE=validation-gatekeeping MODEL=qwen3:4b
make score EXAMPLE=validation-gatekeeping
make report EXAMPLE=validation-gatekeeping
```

## Scoring

### Prompt Injection
- Accuracy, recall (unsafe catch rate), precision, false positive rate
- Risk category accuracy (injection vs jailbreak vs data_exfiltration)

### PII Detection
- Accuracy, recall (PII catch rate), precision
- PII type recall (of expected PII types, how many were correctly identified)

## Files

```
testdata/prompts.json    # 20 user prompts (safe and unsafe)
testdata/pii.json        # 20 texts (with and without PII)
expected/prompts.json    # Ground truth: safe/unsafe + risk category
expected/pii.json        # Ground truth: contains_pii + pii_types
results/                 # Model outputs (generated by running)
```
